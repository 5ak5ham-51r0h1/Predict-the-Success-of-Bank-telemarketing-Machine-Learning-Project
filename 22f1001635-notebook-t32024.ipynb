{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T09:30:09.748245Z",
     "iopub.status.busy": "2024-12-11T09:30:09.747911Z",
     "iopub.status.idle": "2024-12-11T09:30:12.576415Z",
     "shell.execute_reply": "2024-12-11T09:30:12.575438Z",
     "shell.execute_reply.started": "2024-12-11T09:30:09.748217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T09:30:15.299324Z",
     "iopub.status.busy": "2024-12-11T09:30:15.298822Z",
     "iopub.status.idle": "2024-12-11T09:30:15.502504Z",
     "shell.execute_reply": "2024-12-11T09:30:15.501434Z",
     "shell.execute_reply.started": "2024-12-11T09:30:15.299293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T09:30:40.804614Z",
     "iopub.status.busy": "2024-12-11T09:30:40.803731Z",
     "iopub.status.idle": "2024-12-11T09:30:40.915380Z",
     "shell.execute_reply": "2024-12-11T09:30:40.914220Z",
     "shell.execute_reply.started": "2024-12-11T09:30:40.804564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# perform basic EDA\n",
    "print(\"\\nDataset Info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T09:58:45.847599Z",
     "iopub.status.busy": "2024-12-11T09:58:45.846447Z",
     "iopub.status.idle": "2024-12-11T09:58:54.303641Z",
     "shell.execute_reply": "2024-12-11T09:58:54.302425Z",
     "shell.execute_reply.started": "2024-12-11T09:58:45.847550Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine train and test for EDA\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Bar Plot for Target Variable\n",
    "plt.figure(figsize=(8, 4))\n",
    "df['target'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histograms for Key Numerical Features\n",
    "key_numeric_cols = ['age', 'balance', 'duration', 'campaign']\n",
    "for col in key_numeric_cols:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.histplot(df[col], kde=True, color='skyblue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation Heatmap for Key Numerical Features\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[key_numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Key Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box Plot for Key Numerical Features vs. Target\n",
    "for col in key_numeric_cols:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.boxplot(x='target', y=col, data=df, palette='pastel')\n",
    "    plt.title(f'{col} vs Target')\n",
    "    plt.xlabel('Target')\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Stacked Bar Plots for Key Categorical Features\n",
    "key_categorical_cols = ['job', 'marital', 'education', 'poutcome']\n",
    "for col in key_categorical_cols:\n",
    "    cross_tab = pd.crosstab(df[col], df['target'])\n",
    "    cross_tab.plot(kind='bar', stacked=True, figsize=(10, 6), color=['skyblue', 'salmon'])\n",
    "    plt.title(f'Stacked Bar Plot of {col} vs Target')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, test_df], axis=0, sort=False)\n",
    "# Date processing\n",
    "date_columns = df.columns[df.columns.str.contains('date', case=False)]\n",
    "for date_column in date_columns:\n",
    "    try:\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        df[f'{date_column}_month'] = df[date_column].dt.month\n",
    "        df[f'{date_column}_day'] = df[date_column].dt.day\n",
    "        df[f'{date_column}_dayofweek'] = df[date_column].dt.dayofweek\n",
    "        df.drop(date_column, axis=1, inplace=True)\n",
    "    except:\n",
    "        print(f\"Error processing date column {date_column}\")\n",
    "        df.drop(date_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "if 'pdays' in df.columns:\n",
    "    df['was_previously_contacted'] = (df['pdays'] != -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Handling binary variables\n",
    "binary_cols = ['default', 'housing', 'loan']\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map({'yes': 1, 'no': 0, 'unknown': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split back\n",
    "train_df = df[:len(train_df)]\n",
    "test_df = df[len(train_df):]\n",
    "\n",
    "# Prepare Data for Modeling\n",
    "X_train = train_df.drop('target', axis=1)\n",
    "y_train = train_df['target'].map({'yes': 1, 'no': 0})\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing and Train Baseline Model\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "baseline_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_score = baseline_model.score(X_val, y_val)\n",
    "print(f\"Baseline model validation score: {baseline_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Linear Models-SGD with Grid Search and Hyperparameter Tuning\n",
    "sgd_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__loss': ['log_loss', 'modified_huber'],\n",
    "    'classifier__penalty': ['l2', 'l1'],\n",
    "    'classifier__max_iter': [2000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(sgd_model, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best SGD parameters:\", grid_search.best_params_)\n",
    "print(\"Best SGD score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PCA Implementation\n",
    "pca_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pca_pipeline.fit(X_train, y_train)\n",
    "pca_score = pca_pipeline.score(X_val, y_val)\n",
    "print(f\"PCA model validation score: {pca_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "feature_selector = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(score_func=mutual_info_classif, k=10)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "feature_selector.fit(X_train, y_train)\n",
    "selector_score = feature_selector.score(X_val, y_val)\n",
    "print(f\"Feature selection model validation score: {selector_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train Classical Models\n",
    "models = {\n",
    "    'naive_bayes': GaussianNB(),\n",
    "    'knn': KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "    'svm': SVC(kernel='rbf',C=20.0,random_state=42,probability=True)\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_model = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_val, y_val)\n",
    "    print(f\"{name} validation score: {score:.4f}\")\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train Ensemble Models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=2000,\n",
    "        max_depth=25,\n",
    "        min_samples_split=4,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    ('gb', GradientBoostingClassifier(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=10,\n",
    "        subsample=0.9,\n",
    "        min_samples_split=4,\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('mlp', MLPClassifier(\n",
    "        hidden_layer_sizes=(400, 200, 100),\n",
    "        max_iter=5000,\n",
    "        learning_rate='adaptive',\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2,\n",
    "        random_state=42,\n",
    "        alpha=0.00005\n",
    "    ))\n",
    "]\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_models + [\n",
    "        ('svm', models['svm']),\n",
    "        ('knn', models['knn']),\n",
    "        ('nb', models['naive_bayes'])\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(\n",
    "        C=0.18,\n",
    "        max_iter=5000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    cv=7,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "final_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', stacking)\n",
    "])\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "score = final_model.score(X_val, y_val)\n",
    "print(f\"Stacking ensemble validation score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cross-validation scoring\n",
    "'''\n",
    "cv_scores = cross_val_score(final_model, X_train, y_train, cv=5)\n",
    "print(f\"\\nCross-validation scores mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(\"Individual CV scores:\", [f\"{score:.4f}\" for score in cv_scores])\n",
    "'''\n",
    "# final model f-1 score\n",
    "final_pred = final_model.predict(X_val)\n",
    "model_score = f1_score(y_val, final_pred, average='macro')\n",
    "print(f\"Your model's F1 score: {model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate Submission\n",
    "predictions = final_model.predict(test_df)\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(test_df)),\n",
    "    'target': predictions\n",
    "})\n",
    "submission['target'] = submission['target'].map({1: 'yes', 0: 'no'})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"\\nSubmission file 'submission.csv' has been generated.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9578279,
     "sourceId": 85062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
